# GraspNet-1Billion数据集介绍

GraspNet-1Billion数据集包含97280张RGB-D图像和超过10亿个抓取姿势（故称1Billion)。

该数据集包含97280张RGB-D图像，这些图像取自190多个杂乱场景的不同视角，其中每一个场景512张RGB-D图像（512 × 190 = 97280 512\times 190=97280512×190=97280），对于数据集中88个物体提供了精确的3D网格模型，每个场景都密集标注物体的6D pose和抓取姿态。

数据集中共有88种不同物体，从YCB数据集中选取适合抓取的32个物体，从DexNet 2.0中选取13个对抗性对象，并收集自己的43个对象来构建整个物体集，包括洗发水、塑料盆、纸质包装盒等。每种物体均有准确的CAD模型。对每个物体的CAD模型，都可以通过受力分析来使用计算机自动进行标注。

数据集一共有**190**个场景，对于每个场景，本文从整个物体集中随机挑选大约10个物体，将它们随意地堆在一起。数据集将场景1 ∼ 100 定为训练集，101 ∼ 130为“见过的“物体，131 ∼ 160 为”熟悉的”物体，161 ∼ 190为“没见过”的物体。每个场景由两款相机各拍摄的**256**张图片构成，这512张图片是一个机械臂上绑了两个相机拍出来的，机械臂沿着固定的轨迹移动，覆盖四分之一球体上的256个不同视点，因为运动轨迹是确定的，所以**只需要标注第一张图片**，其他都可以通过三维空间中的投影变换得到；而对于第一张图片，也只需要标注物体的姿态，将场景中的物体与模型对齐后，就可以将CAD模型的标注信息投影到RGB-D图像上了。之后需要进行碰撞检查，去掉会产生碰撞的标注。











# 具身智能

具身智能终极目标还是端到端的具身大模型，但过程必须是把过程的每个模块先调通，感知规划控制得分别先调通